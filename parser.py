import requests
from bs4 import BeautifulSoup
import re

headers = {"User-Agent": "Mozilla/5.0"}

def parse_channel_page(channel):
    url = f'https://t.me/s/{channel}'
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    posts = soup.find_all('div', class_='tgme_widget_message_wrap')
    return posts[-1] if posts else None

def extract_text_with_links(tag):
    parts = []

    prev_was_text = False

    for elem in tag.descendants:
        if elem.name == 'br':
            parts.append('\n')
            prev_was_text = False

        elif elem.name == 'a' and 'href' in elem.attrs:
            text = ''.join(elem.stripped_strings)
            href = elem['href']
            link = f'[{text}]({href})'

            # –î–æ–±–∞–≤–∏–º –ø—Ä–æ–±–µ–ª –ø–µ—Ä–µ–¥ —Å—Å—ã–ª–∫–æ–π, –µ—Å–ª–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–π —ç–ª–µ–º–µ–Ω—Ç –±—ã–ª —Ç–µ–∫—Å—Ç–æ–º –∏ –Ω–µ –∑–∞–∫–∞–Ω—á–∏–≤–∞–µ—Ç—Å—è –ø—Ä–æ–±–µ–ª–æ–º/–ø–µ—Ä–µ–Ω–æ—Å–æ–º
            if parts and not re.search(r'[\s\n]$', parts[-1]):
                parts.append(' ')
            parts.append(link)
            prev_was_text = True

        elif isinstance(elem, str):
            # –î–æ–±–∞–≤–∏–º –ø—Ä–æ–±–µ–ª –ø–æ—Å–ª–µ —Å—Å—ã–ª–∫–∏, –µ—Å–ª–∏ –±—ã–ª –∏–¥—É—â–∏–π –ø–æ–¥—Ä—è–¥ —Ç–µ–∫—Å—Ç
            if parts and isinstance(parts[-1], str) and not re.search(r'[\s\n]$', parts[-1]):
                parts.append(' ')
            parts.append(elem)
            prev_was_text = True

    # –£–¥–∞–ª–∏–º –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    result = re.sub(r'[ ]{2,}', ' ', ''.join(parts)).strip()

    return result

def postprocess_text_markdown(text):
    # –°—Å—ã–ª–∫–∏ —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã. –¢–µ–ø–µ—Ä—å –¥–æ–±–∞–≤–∏–º —Ö–µ—à—Ç–µ–≥–∏ –∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è
    text = re.sub(r'(?<!\w)(@\w+)', r'[\1](https://t.me/\1)', text)
    text = re.sub(r'(?<!\w)(#\w+)', r'[\1](https://t.me/hashtag/\1)', text)
    return text

def extract_post_data(post_html, channel):
    text_div = post_html.find('div', class_='tgme_widget_message_text')
    text = extract_text_with_links(text_div)
    text = postprocess_text_markdown(text)

    link_tag = post_html.find('a', class_='tgme_widget_message_date')
    post_link = link_tag['href'] if link_tag else ''
    post_id_match = re.search(r'/(\d+)$', post_link)
    post_id = post_id_match.group(1) if post_id_match else None

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Ñ–æ—Ç–æ
    photo_divs = post_html.find_all('a', class_='tgme_widget_message_photo_wrap')
    photo_urls = []
    for photo_div in photo_divs:
        if 'style' in photo_div.attrs:
            style = photo_div['style']
            match = re.search(r"url\('(.*?)'\)", style)
            if match:
                photo_urls.append(match.group(1))

    # –ü—Ä–∏–∑–Ω–∞–∫ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–∞
    has_file = post_html.find('div', class_='tgme_widget_message_document') is not None
    if not has_file and 'üìé' in text:
        has_file = True

    return {
        'id': post_id,
        'text': text,
        'photo_urls': photo_urls,  # —Ç–µ–ø–µ—Ä—å —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        'link': post_link,
        'channel': channel,
        'has_file': has_file
    }
